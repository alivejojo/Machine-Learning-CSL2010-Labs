# -*- coding: utf-8 -*-
"""vrushali_lab10part`.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Za8adEj_0c2eSNHEp4pA8jyyFBNTBuf2
"""

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

df = pd.read_csv('Iris.csv')
print(df.shape)
df.head()

X = df.drop(columns = ['Id','Species']).to_numpy()
y = df['Species'].to_numpy()

# label encoding on the target column 
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y = encoder.fit_transform(y)
y

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  MLP classifier 
from sklearn.metrics import accuracy_score
#  Vary the number of epochs from 10-100 in a step of 10 and show the loss value curve, using the best set of hyperparameters 
from sklearn.neural_network import MLPClassifier

#  Vary the learning rate for a fixed size of hidden layers and show the best learning rate value when you run it for 50 epochs.
for a in [0.0001,0.001,0.01,0.1,1,10]:
    clf1 = MLPClassifier(hidden_layer_sizes = (150,100), max_iter = 50, activation = 'relu', alpha=a,random_state=0).fit(X_train_scaled,y_train)
    p1 = clf1.predict(X_test_scaled)
    print(a, accuracy_score(p1,y_test))

#  Vary the number of epochs from 10-100 in a step of 10 and show the loss value curve, using the best set of hyperparameters 
from sklearn.model_selection import GridSearchCV
params = {'activation' : ['relu','sigmoid','tanh','identity'] , 'alpha':[0.001,0.01,0.1]}
clf2 = MLPClassifier(hidden_layer_sizes = (150,100))
clf2 = GridSearchCV(clf2,params)
clf2.fit(X_train_scaled,y_train)
print(clf2.best_params_)

loss_list = []
for epoch in [10,20,30,40,50,60,70,80,90,100]:
    clf3 =  MLPClassifier(hidden_layer_sizes = (100,70), max_iter = epoch, activation = 'tanh', alpha=0.001).fit(X_train_scaled,y_train)
    loss_list.append(clf3.loss_)

plt.title("loss against #epochs")
plt.plot([10,20,30,40,50,60,70,80,90,100], loss_list)
plt.show()

y_pred = clf3.predict(X_test_scaled)
print(accuracy_score(y_test,y_pred))

# -*- coding: utf-8 -*-
"""B20BB047_Lab10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TevUqF9Mlh-v0EtLW6CteAGxppLhFaiU
"""

#importing libraries
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns

import random

df_train = pd.read_csv('train_house.csv')
print(df_train.shape)
df_train.head()

df_test = pd.read_csv('test_house.csv')
print(df_test.shape)
df_test.head()

"""# Pre processing

1. Missing vals
2. numerical and categorical variable 
3. distribution of numerical variables
4. handling outliers and missing values
"""

# missing vals 
null_features = [f for f in df_train.columns if df_train[f].isnull().sum()>1]
null_features_percentage = [ (df_train[f].isnull().sum()/1459)*100 for f in df_train.columns if df_train[f].isnull().sum()>1]
for i in range(len(null_features)):
    print(null_features[i], "percent null value: ", null_features_percentage[i])

# PoolQC,Alley,Fence and MiscFeature are pretty much only null values. we can drop them 
df_train = df_train.drop(columns=['Id','PoolQC','MiscFeature','Alley','Fence'])

null_features.remove('PoolQC')
null_features.remove('MiscFeature')
null_features.remove('Alley')
null_features.remove('Fence')

null_features

# impute NaN values with the average value of that feature
from sklearn.impute import SimpleImputer

imp = SimpleImputer()
for i in null_features:
    try:
        df_train[i] = imp.fit_transform(df_train[i].values.reshape(-1, 1))
    except:
        pass

df_train.loc[:, df_train.isnull().any()].columns

'''
these are null values of 
Index(['MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
       'BsmtFinType2', 'Electrical', 'FireplaceQu', 'GarageType',
       'GarageFinish', 'GarageQual', 'GarageCond'],
      dtype='object')
these are string based categorical data. 

We first see how much they actually contribute to SalePrice.
'''

# if we look at the data desciption we find that of all the stringy categorical data only MasVnrType needs to be cleaned. Rest all null values are not noise but mean the absence of some amenity in the homes. 
# this can be done by taking a random realization of the feature
# eg. BrkFace

for j in range(len(df_train['MasVnrType'].values)):
    if df_train['MasVnrType'].values[j] == 'None':
        df_train['MasVnrType'].values[j]='BrkFace'
df_train['MasVnrType']

print(list(df_train.dtypes))

"""Here the dtype('o') is obejct. -> categorical data."""

# some features are based on time -> Year or Yr ones which are numerical but dont mean anything numeric 
time_features = [f for f in df_train.columns if 'Yr' in f or 'Year' in f]

# now suppose YrSold. 
# YrSold alone is not really informative, because the house maybe sold now but may have extremely old Year of build
df_train.groupby('YrSold')['SalePrice'].mean().plot()
plt.title(" yrsold vs the mean price of houses in that ")
plt.show()

for i in time_features:
    if i!= 'YrSold':
        y=df_train['YrSold']-df_train[i]
        plt.scatter(y,df_train['SalePrice'])
        plt.title(i)
        plt.show()

"""These plots show that as the difference between the year of building, year of remodelling and year of builiding garage with the year of selling decreases, the price of the house increases. 

Which makes sense. 
"""

'''
2 types of features:
    1. discrete (fixed numerical values. subset = temporal or time_features and categorical (nominal and ordinal))
    2. continuous
'''
numerical_features = [f for f in df_train.columns if df_train[f].dtype!='O']
discrete_features = [f for f in numerical_features if len(df_train[f].unique())<25 and f not in time_features]
continuous_features = [f for f in numerical_features if f not in discrete_features]
categorical_features = [f for f in df_train.columns if df_train[f].dtype=='O']

categorical_features

from sklearn.preprocessing import LabelEncoder
df_train2 = df_train.copy()

encoder = LabelEncoder()
for i in categorical_features:
    df_train2[i] = encoder.fit_transform(df_train[i])

df_train2[categorical_features]

# as we saw earlier with the temporal variables that just the stand alone features didnt mean much
# we have to take their differnces with the year sold. 
for i in time_features:
    if i!='YearBuilt':
        df_train2[i] = df_train2[i]-df_train2['YearBuilt']
df_train2 = df_train2.drop(columns=['YearBuilt'])

"""# dealing with outliers"""

# idk T_T

"""### Train an MLP model (with at least one hidden layer, and an appropriate non-linear activation function if necessary) using training data by varying different hyperparameters, and predict the sales price of the houses."""

X = df_train2.drop(columns=['SalePrice'])
y = df_train2['SalePrice']

# splitting the train set because one question asks for truth values of test data which we dont have
from sklearn.model_selection import train_test_split

X_train, X_test , y_train, y_test = train_test_split(X,y,test_size = 0.3)

# changing the labels to float from int
y_train = y_train.astype('float64')
y_test = y_test.astype('float64')

# before training, we must standardize all variables.
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.neural_network import MLPRegressor
# Train an MLP model (with at least one hidden layer, and an appropriate non-linear activation function if necessary)
# using training data by varying different hyperparameters, and predict the sales price of the houses.

from sklearn.metrics import accuracy_score


regressor = MLPRegressor(hidden_layer_sizes=(150,100), activation = 'relu', max_iter = 1000).fit(X_train,y_train)
test_prediction = regressor.predict(X_test)
#finding the MSE for prediction

from sklearn.metrics import mean_squared_error

# mse = mean_squared_error()
print(mean_squared_error(y_test, test_prediction))

for i in range(len(X_test)):
    if i<=4:
        print("features: ", X_test[i], "price: " ,test_prediction[i], '\n')

from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances

# y_test = y_test.to_numpy()
# print("euclidean distance = ", euclidean_distances(y_test,test_prediction))
y_test = list(y_test)
print(len(test_prediction))

print(y_test)

def euclidean(a,b):
    d=0
    for i in range(len(a)):
        d=d+(a[i]-b[i])**2
    return d**(1/2)

print(euclidean(y_test,test_prediction))

def manhattan(a,b):
    d=0
    for i in range(len(a)):
        d=d+abs(a[i]-b[i])
    return d

print(manhattan(y_test,test_prediction))

'''
1188393491.0447512 -> MSE
721468.189927734   -> euclidean 
10639863.598244008 -> mamhattan
'''

# Plot the loss curve, and a histogram of prediction errors with an appropriate bin size. 
# for this we need to make a list of all the errors

def euclidean_2(a,b):
    d=[]
    for i in range(len(a)):
        d.append((a[i]-b[i])**2)
    for i in range(len(d)):
        d[i] = d[i]**(1/2)
    return d

def manhattan_2(a,b):
    d=[]
    for i in range(len(a)):
        d.append(abs(a[i]-b[i]))
    return d

euclidean_error_list = euclidean_2(y_test, test_prediction)
manhattan_error_list = manhattan_2(y_test, test_prediction)
plt.hist(euclidean_error_list,bins=100)
plt.title('euclidean loss')
plt.show()
plt.hist(manhattan_error_list,bins=100)
plt.title('manhattan loss')

plt.show()
