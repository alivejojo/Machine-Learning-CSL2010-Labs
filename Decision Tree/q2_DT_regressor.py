# -*- coding: utf-8 -*-
"""lab5_task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B0a09X_2A5z_P_5H1cWNVtITLbBLfLkK
"""

import pandas as pd
from google.colab import files
uploaded = files.upload()


df = pd.read_csv("Concrete_Data.csv")
df.head()


from sklearn.model_selection import train_test_split

train_, test_ = train_test_split( df, test_size=0.30, random_state=2021)
X_train = train_.iloc[:,:-1]
y_train = train_.iloc[:,-1]
X_test = test_.iloc[:,:-1]
y_test = test_.iloc[:,-1]

X_train.head()




from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
for i in [17,18,19,20,22,25,30]:
  regressor = DecisionTreeRegressor(random_state=2021, min_samples_split =i , splitter = 'random')
  regressor.fit(X_train, y_train)
  pred_y = regressor.predict(X_train)
  print(mean_squared_error(y_train,pred_y))

'''
41.1024162994163
39.667802768974354
48.721972886070986
61.423822343396466
85.45646357516895
'''
# implies that if an internal node has <20 samples then further no splitting will happen
# done to prevent overfitting




regressor = DecisionTreeRegressor(random_state=2021, min_samples_split = 17, splitter = 'random')
regressor.fit(X_train,y_train)
pred_train_Y = regressor.predict(X_train)

from sklearn.metrics import mean_squared_error, mean_absolute_error
print("Mean squared error in training = " , mean_squared_error(y_train,pred_train_Y))
print("Mean absolute error in training = " , mean_absolute_error(y_train,pred_train_Y))
pred_test_Y = regressor.predict(X_test)
print("Mean squared error in testing = " , mean_squared_error(y_test,pred_test_Y))
print("Mean absolute error in testing= " , mean_absolute_error(y_test,pred_test_Y))



import graphviz
from sklearn import tree

dot_tree = tree.export_graphviz(regressor,filled=True,rounded=True)
graph = graphviz.Source(dot_tree, format="png") 
graph

graph.render("decision_tree_graphivz")
from google.colab import files
files.download("decision_tree_graphivz.png")



# now using MAE
for i in [15,17,20,25]:
  regressor2 = DecisionTreeRegressor(criterion = 'mae', random_state=2021, min_samples_split =i , splitter = 'random')
  regressor2.fit(X_train, y_train)
  print()
  pred_train_y = regressor2.predict(X_train)
  pred_test_y = regressor2.predict(X_test)
  print(i)
  print("Mean squared error in training = " , mean_squared_error(y_train,pred_train_y))
  print("Mean absolute error in training = " , mean_absolute_error(y_train,pred_train_y))

  print("Mean squared error in testing = " , mean_squared_error(y_test,pred_test_y))
  print("Mean absolute error in testing= " , mean_absolute_error(y_test,pred_test_y))

# min sample split =15 outperforms other..

# chossing min_sample_split = 17 as the best model
# note we cant make the confusion matrix or accuracy report as this is a regression task and not a classification task .

# MSE model performs better than MSE
