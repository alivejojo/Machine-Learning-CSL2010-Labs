# -*- coding: utf-8 -*-
"""B20BB047_lab8_task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAzGSBoUi5_G-3ViNrqiOwDbMkOyJCbm

##  **TASK 2: part 1**
Hierarchical Clustering and K Means [w/o inbuilt function]
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('milk.csv')

print(df.shape)
df.head()

X = df.drop(columns = 'Unnamed: 0').to_numpy()

# clustering is un-supervised hence we dont really need the y. 
from sklearn.cluster import KMeans

model = KMeans(n_clusters = 3, init='random',n_init=10, max_iter=300, random_state=2021)

pred_label = model.fit_predict(X)

pred_label

plt.scatter(X[:,0], X[:,1], c = pred_label)
plt.scatter(model.cluster_centers_[:,0], model.cluster_centers_[:,1],  s=100, marker='o', c='black')
plt.show()

#Choosing right value of k
loss = []
for i in range(1, 11):
    km = KMeans(
        n_clusters=i, init='random',
        n_init=10, max_iter=300,
        tol=1e-04, random_state=0
    )
    km.fit(X)
    loss.append(km.inertia_)

# plot
plt.plot(range(1, 11), loss, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Loss')
plt.show()

"""the Elbow-Plot shows that number of clusters = 3 is the best choice

### Dendrogram
"""

# dendogram 
from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(X , 'average')
labelList = range(1, 26)
plt.figure(figsize=(10, 7))
dendrogram(linked,
            orientation='top',
            labels=labelList
          )
plt.title('average linkage')
plt.show()

linked = linkage(X , 'ward')
labelList = range(1, 26)
plt.figure(figsize=(10, 7))
dendrogram(linked,
            orientation='top',
            labels=labelList
          )
plt.title('ward linkage')
plt.show()

"""### Clustering for different kinds of Linkages = {'average' , 'complete'}




"""

from sklearn.cluster import AgglomerativeClustering

clustering = AgglomerativeClustering(n_clusters = 3, linkage = 'average')
pred_y_2 = clustering.fit_predict(X)

plt.scatter(X[:,0],X[:,1], c = pred_y_2)
plt.title("agglomerative clustering with k=3 and linkage = average")
plt.show()

clustering2 = AgglomerativeClustering(n_clusters = 4, linkage = 'complete')
pred_y_3 = clustering2.fit_predict(X)
plt.scatter(X[:,0], X[:,1], c = pred_y_3)
plt.title("agglomerative clustering with k=4 and linkage = complete")

plt.show()

clustering3 = AgglomerativeClustering(n_clusters = 4, linkage = 'average')
pred_y_4 = clustering3.fit_predict(X)
plt.scatter(X[:,0], X[:,1], c = pred_y_4)
plt.title("agglomerative clustering with k=4 and linkage = average")

plt.show()

"""##  **TASK 2: part 2**
Kmeans algorithm from Scratch

Kmeans is a distance based algorithm. Hence to remove any un-wanted bias, we must standard scale the data.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X=scaler.fit_transform(X)

def euclidean(x1,x2):
    # calculates euclidean distance between to vectors i.e between two samples. 
    d=0
    d = np.sqrt(np.sum(np.square(x1 - x2)))
    return d

def closest_class(x, cluster_centres):

    # Function which when given a vector and a set of cluster centres, returns the cluster centre closest to the vector
    d = []
    for i in range(len(cluster_centres)):
        d.append(euclidean(x, cluster_centres[i]))
    return d.index(min(d))

def mean_of_classes(X,y):

    # function which when given all the samples (X) and their repsective labels (y) gives the mean of samples - classwise. 
    classes,count_classes = np.unique(y, return_counts=True)
    mean_of_classes = np.zeros((len(classes), 5))

    count_classes = np.insert(count_classes, 0, 0)
    
    for i in range(1,len(count_classes)):
        count_classes[i] += count_classes[i-1]

    for i in range(len(count_classes)-1):
        mean_of_classes[i] = np.mean(X[count_classes[i]:count_classes[i+1]],axis=0)
    
    return mean_of_classes

def kmeans(k,X):

    classes = np.arange(0,k)
    ind = np.random.choice(len(X), size = k, replace = False)
    cluster_centres = X[ind,:]

    cluster_centres_updated = np.zeros(cluster_centres.shape)
    y = np.zeros((len(X),))

    #initial classes
    for i in range(len(X)):
        y[i] = closest_class(X[i], cluster_centres)


    #updating classes
    '''
    ASSUMPTION: 
    ===========
    Because K-means is an iterative algorithm, we can't be sure that it will EXACTLY converge where previous and updated cluster centres overlap exactly.\
    This will eventually happen, however a close approximation is often good enough and can save computation 
    Hence I have used dist(previous cluster centres, updated cluster centres)==0.005 as the convergence criterion
    '''
    while euclidean(cluster_centres, cluster_centres_updated) >= 0.005:
        cluster_centres = cluster_centres_updated
        cluster_centres_updated = mean_of_classes(X,y)
        for i in range(len(X)):
            y[i] = closest_class(X[i], cluster_centres_updated)

    return cluster_centres_updated,y

cluster_Centre_final, y = kmeans(3,X)

plt.scatter(X[:,0],X[:,1],c=y)
plt.show()
