# -*- coding: utf-8 -*-
"""IML LAB 9_Vrushali.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gZzP6rvNPPpAlqBBO43pEPqT9KoGB4_F

# Importing and Pre-processing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('diabetes (1).csv')
print(df.shape)
df.head()

X = df.drop(columns= ['Outcome']).to_numpy()
y = df['Outcome'].to_numpy()

y

def check_zeros(clm):

  s=0
  for i in range(len(clm)):
    if clm[i] == [0]: 
      s = s + 1
  return s

print("number of zeros in : ")
for i in list(df.columns):
  
  print(i, " -> ", check_zeros(   df[[i]].to_numpy()  ))

# zeros in Pregnancies and Outcomes makes sense. 
# 0's in Glucose, BloodPressure, SkinThickness, BMI don't make sense
# we impute them by mean values of the respective features

def impute_by_mean (clm):

  for i in range(df[clm].values.shape[0]):
    if df[clm].values[i] == 0:
      df[clm].values[i] = df[clm].mean()

impute_by_mean('Glucose')
impute_by_mean('BloodPressure')
impute_by_mean('SkinThickness')
impute_by_mean('BMI')

print("number of zeros in : ")
for i in list(df.columns):
  
  print(i, " -> ", check_zeros(   df[[i]].to_numpy()  ))

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# II)

"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

dim_reducer1 = LinearDiscriminantAnalysis( solver = 'svd', n_components = 1) 
projected_X = dim_reducer1.fit(X_scaled,y).transform(X_scaled)

projected_X.shape

plt.scatter(np.arange(768), projected_X, c = y)
plt.show()

# the graph shows the best seperation based on LDA in one dimension. The original space is now projected onto a line.
# this is because ```Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction```

"""## PCA:"""

from sklearn.decomposition import PCA

pca = PCA(.95)
projected_X_PCA = pca.fit_transform(X_scaled)

pca.n_components_

y = y.reshape((768,1))

# Applying K-nn on splitted data
from sklearn.model_selection import train_test_split

X_train_lda, X_test_lda, y_train_lda, y_test_lda = train_test_split(projected_X, y, test_size = 0.3, random_state = 0)
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(projected_X_PCA, y, test_size = 0.3, random_state = 0)

from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.metrics import accuracy_score 

knn_lda = KNN(n_neighbors=5)
knn_lda.fit(X_train_lda,y_train_lda)
y_pred_lda = knn_lda.predict(X_test_lda)

print("accuracy of knn with n_neighbours = 5 on LDa" , accuracy_score(y_test_lda,y_pred_lda))

X_train_lda.shape

# applying PCA on the same with different components (first the highest variance or 0th component of PCA, then the 1st component and so on)
pca_accuracies = []

# Using knn for classification using the reduced dimensions of PCA
knn_pca = KNN(n_neighbors = 5)
knn_pca.fit(X_train_pca[:,0].reshape((537,1)),y_train_pca)
y_pred_pca = knn_pca.predict(X_test_pca[:,0].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the highest variance component" , accuracy_score(y_test_pca,y_pred_pca))
pca_accuracies.append( accuracy_score(y_test_pca,y_pred_pca))

# with the 2nd highest varianve retained component i.e -> the using the second column of reduced dimension of PCA

knn_pca1 = KNN(n_neighbors=5)
knn_pca1.fit(X_train_pca[:,1].reshape((537,1)), y_train_pca)
y_pred_pca1 = knn_pca.predict(X_test_pca[:,1].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the second highest variance component" , accuracy_score(y_test_pca,y_pred_pca1))
pca_accuracies.append( accuracy_score(y_test_pca,y_pred_pca1))

knn_pca2 = KNN(n_neighbors=5)
knn_pca2.fit(X_train_pca[:,2].reshape((537,1)), y_train_pca)
y_pred_pca2 = knn_pca.predict(X_test_pca[:,2].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the third highest variance component" , accuracy_score(y_test_pca,y_pred_pca2))
pca_accuracies.append(accuracy_score(y_test_pca,y_pred_pca2))

knn_pca3 = KNN(n_neighbors=5)
knn_pca3.fit(X_train_pca[:,3].reshape((537,1)), y_train_pca)
y_pred_pca3 = knn_pca.predict(X_test_pca[:,3].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the 4th highest variance component" , accuracy_score(y_test_pca,y_pred_pca3))
pca_accuracies.append(accuracy_score(y_test_pca,y_pred_pca3))

knn_pca4 = KNN(n_neighbors=5)
knn_pca4.fit(X_train_pca[:,4].reshape((537,1)), y_train_pca)
y_pred_pca4 = knn_pca.predict(X_test_pca[:,4].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the 5th highest variance component" , accuracy_score(y_test_pca,y_pred_pca4))
pca_accuracies.append(accuracy_score(y_test_pca,y_pred_pca4))

knn_pca5 = KNN(n_neighbors=5)
knn_pca5.fit(X_train_pca[:,5].reshape((537,1)), y_train_pca)
y_pred_pca5 = knn_pca.predict(X_test_pca[:,5].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the 6th highest variance component" , accuracy_score(y_test_pca,y_pred_pca5))
pca_accuracies.append(accuracy_score(y_test_pca,y_pred_pca5))

knn_pca6 = KNN(n_neighbors=5)
knn_pca6.fit(X_train_pca[:,6].reshape((537,1)), y_train_pca)
y_pred_pca6 = knn_pca.predict(X_test_pca[:,6].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the 7th highest variance component" , accuracy_score(y_test_pca,y_pred_pca6))
pca_accuracies.append(accuracy_score(y_test_pca,y_pred_pca6))

knn_pca7 = KNN(n_neighbors=5)
knn_pca7.fit(X_train_pca[:,7].reshape((537,1)), y_train_pca)
y_pred_pca7 = knn_pca.predict(X_test_pca[:,7].reshape((231,1)))

print("accuracy of knn with n_neighbours = 5 on pca on the 8th highest variance component" , accuracy_score(y_test_pca,y_pred_pca7))
pca_accuracies.append(accuracy_score(y_test_pca,y_pred_pca7))

plt.title("accuracies of Classification using KNN on projected vectors using PCA")
plt.plot(np.arange(len(pca_accuracies)), pca_accuracies)
plt.show()

# hmm why?

"""# III)



"""

# making a dataframe with first column as prediction of LDA and 7 other columns 
# then make 7 different scatter matrices for LDA prediction and the respective column of PCA. 
# all for predictions on the test dataset.

df_compare = pd.DataFrame(y_pred_lda)

df_compare.head()

y_compare = np.concatenate(( y_pred_pca.reshape((231,1)), y_pred_pca1.reshape((231,1)), y_pred_pca2.reshape((231,1)), y_pred_pca3.reshape((231,1)), y_pred_pca4.reshape((231,1)), y_pred_pca5.reshape((231,1)), y_pred_pca6.reshape((231,1)) ), axis = 1)

df_compare2 = pd.DataFrame(pd.np.column_stack([df_compare,y_compare]))

df_compare2.head()

df_compare2.columns = ['LDA', 'PCA_1st_component', 'PCA_2nd_component', 'PCA_3rd_component', 'PCA_4th_component', 'PCA_5th_component', 'PCA_6th_component', 'PCA_7th_component' ]
df_compare2.head()

for i in range(1,8):
  df3 = pd.concat([df_compare2.iloc[0], df_compare2.iloc[i]], axis = 1)
  pd.plotting.scatter_matrix(df3)

"""# IV)"""

train_x, test_x, train_y, test_y = train_test_split(X_scaled, y, test_size = 0.3)

# train decision tree on the train data and find accuracy on the test data) 
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(train_x, train_y)
logreg_pred = logreg.predict(test_x)
print(accuracy_score(test_y, logreg_pred ))

# now we train another logistic regressor on the feature space returned by LDA:
logreg2 = LogisticRegression()
logreg2.fit(X_train_lda, y_train_lda)
logreg2_pred_lda = logreg2.predict(X_test_lda)
print(accuracy_score(y_test_lda, logreg2_pred_lda))

# we can see that the features extracted by LDA perform better, even though the number of features is lesser.

"""# V)"""

from sklearn.neural_network import MLPClassifier

for nodes in [10,30,50,70,90]:
  for ac in ['identity','logistic','tanh','relu']:
    clf = MLPClassifier(random_state=42, max_iter=800, activation = ac, hidden_layer_sizes= (nodes)).fit(train_x, train_y)
    pred_lab = clf.predict(test_x)
    print(nodes, " ",ac, accuracy_score(test_y, pred_lab))

# the best accuracy is attained with #nodes = 10, and with 'identity' activation function
# this accuracry is higher than that of using Logistic Regression because Multi-Layer-Perceptrons have the ability to learn complex functions 
# also notice that with more number of nodes, the testing acc-- becasue of overfitting
